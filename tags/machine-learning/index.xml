<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | </title>
    <link>https://Likhitha96.github.io/tags/machine-learning/</link>
      <atom:link href="https://Likhitha96.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 01 Oct 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://Likhitha96.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Machine Learning</title>
      <link>https://Likhitha96.github.io/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>Distributed Stochastic Gradient Descent</title>
      <link>https://Likhitha96.github.io/project/project2/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://Likhitha96.github.io/project/project2/</guid>
      <description>&lt;p&gt;Gradient descent is an optimization algorithm used to find the minimum of a function.  By iterative moving in the direction of the steepest descent, we arrive at a minimum of the cost function.
Gradient descent is used in a lot of machine  learning techniques to update model parameters, be it the coefficient in linear regression or the weights in neural networks.
While stochastic gradient descent calculates gradient for seeing each data point and updates the parameters each time, parallelizing SGD is an approach to leverage the effectiveness of stochastic gradient descent but also reduction in time.&lt;br /&gt;
It also satisfies the requirement of huge computational power in a distributed fashion and reduce load on a single machine.&lt;/p&gt;

&lt;p&gt;The motivation behind this project is to explore the current distributed stochastic gradient descent algorithms and compare their performance between different datasets.&lt;br /&gt;
It intends to examine the sensitivity of different datasets to different algorithms.&lt;/p&gt;

&lt;p&gt;Techincal tools: Python, Apache Spark&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
